{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract\n",
    "\n",
    "Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "SOURCE_HOST = \"tau\"\n",
    "\n",
    "\n",
    "def read_csvs(directory: str) -> pd.DataFrame:\n",
    "    files = sorted(glob.glob(os.path.join(directory, \"*.csv\")))\n",
    "    df_list = [pd.read_csv(file, index_col=0, parse_dates=True) for file in files]\n",
    "    return pd.concat(df_list).sort_index()\n",
    "\n",
    "\n",
    "df = read_csvs(f\"../../input_data/{SOURCE_HOST}/tracepoints/net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[\"2025-01-24\":]  # discard IBD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any filter* messages?\n",
    "spv_msgs = [msg for msg in df.msg_type.unique().tolist() if msg.startswith(\"filter\")]\n",
    "print(spv_msgs)\n",
    "\n",
    "# investigate SPV peer\n",
    "display(df[df[\"msg_type\"] == \"filterload\"])\n",
    "spv_peers = df[df[\"msg_type\"] == \"filterload\"][\"peer_addr\"].unique().tolist()\n",
    "for spv_peer in spv_peers:\n",
    "    display(df[df[\"peer_addr\"] == spv_peer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect, Develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_backup = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# from pandarallel import pandarallel\n",
    "\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "\n",
    "# Group by 'peer_id' and 'peer_addr', then aggregate the required metrics\n",
    "result = (\n",
    "    df.groupby([\"peer_id\", \"peer_addr\"])\n",
    "    .apply(\n",
    "        lambda g: pd.Series(\n",
    "            {\n",
    "                \"connection_time\": g.index.max() - g.index.min(),\n",
    "                \"in_size\": g.loc[g[\"flow\"] == \"in\", \"size\"].sum(),\n",
    "                \"out_size\": g.loc[g[\"flow\"] == \"out\", \"size\"].sum(),\n",
    "                \"total_size\": g[\"size\"].sum(),\n",
    "                \"in_count\": g.loc[g[\"flow\"] == \"in\", \"size\"].sum(),\n",
    "                \"out_count\": g.loc[g[\"flow\"] == \"out\", \"size\"].sum(),\n",
    "                \"total_count\": g[\"size\"].sum(),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# # Suppose your dataframe is called df. First reset the index:\n",
    "# df = df.reset_index()  # so that 'timestamp' is a column, not the index\n",
    "\n",
    "# # Group and aggregate\n",
    "# grouped = df.groupby([\"peer_id\", \"peer_addr\"], dropna=False)\n",
    "\n",
    "# result = grouped.agg(\n",
    "#     first_timestamp=(\"timestamp\", \"min\"),\n",
    "#     last_timestamp=(\"timestamp\", \"max\"),\n",
    "#     in_size=(\"size\", lambda s: s[df.loc[s.index, \"flow\"] == \"in\"].sum()),\n",
    "#     out_size=(\"size\", lambda s: s[df.loc[s.index, \"flow\"] == \"out\"].sum()),\n",
    "#     total_size=(\"size\", \"sum\"),\n",
    "#     in_count=(\"size\", lambda s: s[df.loc[s.index, \"flow\"] == \"in\"], \"count\"),\n",
    "#     out_count=(\"size\", lambda s: s[df.loc[s.index, \"flow\"] == \"out\"], \"count\"),\n",
    "#     total_count=(\"size\", \"count\"),\n",
    "# ).reset_index()\n",
    "\n",
    "# # Compute the connection_time from first and last timestamps\n",
    "# result[\"connection_time\"] = result[\"last_timestamp\"] - result[\"first_timestamp\"]\n",
    "\n",
    "# # print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# result.info()\n",
    "\n",
    "# 1. Filter rows where connection_time < 1 hour\n",
    "one_hour = pd.Timedelta(hours=1)\n",
    "filtered = result[result[\"connection_time\"] >= one_hour].copy()\n",
    "\n",
    "duration_seconds = filtered[\"connection_time\"].dt.total_seconds()\n",
    "duration_hours = filtered[\"connection_time\"].dt.total_seconds() / 3600\n",
    "\n",
    "filtered[\"bw_in\"] = filtered[\"in_size\"] / duration_seconds\n",
    "filtered[\"bw_out\"] = filtered[\"out_size\"] / duration_seconds\n",
    "filtered[\"bw_total\"] = filtered[\"total_size\"] / duration_seconds\n",
    "\n",
    "filtered[\"mph_in\"] = filtered[\"in_count\"] / duration_hours\n",
    "filtered[\"mph_out\"] = filtered[\"out_count\"] / duration_hours\n",
    "filtered[\"mph_ratio_in_out\"] = filtered[\"mph_in\"] / filtered[\"mph_out\"]\n",
    "filtered[\"mph_total\"] = filtered[\"total_count\"] / duration_hours\n",
    "\n",
    "# 3. Sort by connection_time in ascending order\n",
    "filtered = filtered.sort_values(by=\"mph_ratio_in_out\", ascending=False)\n",
    "\n",
    "print(len(filtered))\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metric = \"bw_total\"\n",
    "\n",
    "# 1. Sort by bw_total\n",
    "filtered_sorted = filtered.sort_values(metric).reset_index(drop=True)\n",
    "\n",
    "# 2. Plot the sorted values\n",
    "plt.figure()\n",
    "plt.plot(filtered_sorted[metric], marker=\"o\", linestyle=\"\")  # scatter-like\n",
    "# or use plt.scatter(range(len(filtered_sorted)), filtered_sorted['bw_total'])\n",
    "\n",
    "plt.xlabel(\"Row index after sorting\")\n",
    "plt.title(metric)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Share analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# If your data is not exactly aligned with these columns,\n",
    "# you'll need to rename or adapt accordingly:\n",
    "# Columns assumed: peer_id, peer_addr, flow, msg_type, size, and a DateTimeIndex 'timestamp'.\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2) First, group by (peer_id, peer_addr)\n",
    "#    and compute the connection duration\n",
    "# -------------------------------------------------------\n",
    "group_cols = [\"peer_id\", \"peer_addr\"]\n",
    "\n",
    "\n",
    "# We'll define a helper to calculate the duration\n",
    "def connection_duration(g):\n",
    "    return g.index.max() - g.index.min()\n",
    "\n",
    "\n",
    "# Compute durations for each group\n",
    "durations = df.groupby(group_cols).apply(connection_duration)\n",
    "\n",
    "# durations is a Series with (peer_id, peer_addr) as index, and Timedelta as the value.\n",
    "# Keep only groups > 1 hour\n",
    "valid_pairs = durations[durations > pd.Timedelta(hours=1)].index\n",
    "\n",
    "# Filter the main DataFrame to only include those valid pairs\n",
    "df_filtered = df[df.set_index(group_cols).index.isin(valid_pairs)]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3) Discard all rows where flow == 'out'\n",
    "#    i.e., keep only inbound messages\n",
    "# -------------------------------------------------------\n",
    "df_inbound = df_filtered[df_filtered[\"flow\"] != \"out\"]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4) Now, for the remaining inbound-only rows, we want\n",
    "#    to group by (peer_id, peer_addr) and msg_type\n",
    "#    to compute:\n",
    "#      - Count of each msg_type\n",
    "#      - The share of each msg_type\n",
    "# -------------------------------------------------------\n",
    "# First, compute total counts per group & msg_type\n",
    "counts = df_inbound.groupby(group_cols + [\"msg_type\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "# Next, compute the total count *per group* so we can get the share\n",
    "group_sums = counts.groupby(group_cols)[\"count\"].transform(\"sum\")\n",
    "\n",
    "# Add a 'share' column to counts\n",
    "counts[\"share\"] = counts[\"count\"] / group_sums\n",
    "\n",
    "# Now we have a table with columns:\n",
    "#   peer_id, peer_addr, msg_type, count, share\n",
    "# If we want each msg_type's share as its own column, we can pivot.\n",
    "result = counts.pivot_table(\n",
    "    index=group_cols, columns=\"msg_type\", values=\"share\", fill_value=0\n",
    ")\n",
    "\n",
    "# Optionally rename columns to have a suffix \"_share\"\n",
    "result = result.add_suffix(\"_share\").reset_index()\n",
    "\n",
    "# 'result' is now a DataFrame:\n",
    "#   peer_id, peer_addr, <msg_type1>_share, <msg_type2>_share, ...\n",
    "# For only the pairs that had >1 hour connections and inbound messages.\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5) (Optional) If you also want the absolute counts in the same table,\n",
    "#    you can pivot on the 'count' as well, or merge them\n",
    "# -------------------------------------------------------\n",
    "counts_pivot = counts.pivot_table(\n",
    "    index=group_cols, columns=\"msg_type\", values=\"count\", fill_value=0\n",
    ")\n",
    "counts_pivot = counts_pivot.add_suffix(\"_count\").reset_index()\n",
    "\n",
    "# Merge shares and counts if desired\n",
    "final = pd.merge(result, counts_pivot, on=group_cols, how=\"left\")\n",
    "\n",
    "# 'final' now has both share and count columns for each msg_type.\n",
    "# e.g.  peer_id, peer_addr, getheaders_share, headers_share, ping_share, ...\n",
    "#       getheaders_count, headers_count, ping_count, ...\n",
    "\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1) Sort your DataFrame by inv_share, tx_share (example)\n",
    "# df_sorted = final.sort_values(by=[\"inv_share\", \"ping_share\"])\n",
    "df_sorted = final.sort_values(by=[\"getdata_share\", \"ping_share\"])\n",
    "\n",
    "# 2) Select columns that end with \"_share\"\n",
    "share_cols = [col for col in df_sorted.columns if col.endswith(\"_share\")]\n",
    "df_shares = df_sorted[share_cols].reset_index(drop=True)\n",
    "\n",
    "# 3) Filter out columns whose minimum value ≤ 0.01\n",
    "valid_cols = [col for col in df_shares.columns if df_shares[col].max() > 0.001]\n",
    "df_shares_filtered = df_shares[valid_cols]\n",
    "\n",
    "# 4) Generate a color list using \"tab20\"\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "cmap = cm.get_cmap(\"tab20\")  # tab20 has 20 distinct colors\n",
    "n_colors = len(valid_cols)\n",
    "\n",
    "# We'll cycle through the 0..1 range in small increments if we have many columns\n",
    "colors = [cmap((i % 20) / 19.0) for i in range(n_colors)]\n",
    "\n",
    "# 5) Plot a stacked area chart with the filtered columns\n",
    "ax = df_shares_filtered.plot.area(stacked=True, figsize=(12, 6), color=colors)\n",
    "ax.legend(bbox_to_anchor=(0.5, 1.1), loc=\"upper center\", ncol=5, title=\"\", fontsize=6)\n",
    "\n",
    "ax.set_xlabel(\"Row index (each row is a peer connection)\")\n",
    "ax.set_ylabel(\"Share\")\n",
    "ax.set_title(\"Stacked Area Chart of Message Type Shares (Filtered)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP addresses of sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.peer_addr.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate zero-inv nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final2 = final[final[\"inv_count\"] == 0]\n",
    "# 1) Sort your DataFrame by inv_share, tx_share (example)\n",
    "df_sorted = final2.sort_values(by=[\"getheaders_share\", \"pong_share\"])\n",
    "\n",
    "# 2) Select columns that end with \"_share\"\n",
    "share_cols = [col for col in df_sorted.columns if col.endswith(\"_share\")]\n",
    "df_shares = df_sorted[share_cols].reset_index(drop=True)\n",
    "\n",
    "# 3) Filter out columns whose minimum value ≤ 0.01\n",
    "valid_cols = [col for col in df_shares.columns if df_shares[col].max() > 0.001]\n",
    "# valid_cols = [col for col in df_shares.columns ]\n",
    "print(valid_cols)\n",
    "df_shares_filtered = df_shares[valid_cols]\n",
    "\n",
    "\n",
    "# 4) Generate a color list using \"tab20\"\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "cmap = cm.get_cmap(\"tab20\")  # tab20 has 20 distinct colors\n",
    "n_colors = len(valid_cols)\n",
    "\n",
    "# We'll cycle through the 0..1 range in small increments if we have many columns\n",
    "colors = [cmap((i % 20) / 19.0) for i in range(n_colors)]\n",
    "\n",
    "# 5) Plot a stacked area chart with the filtered columns\n",
    "ax = df_shares_filtered.plot.area(stacked=True, figsize=(12, 6), color=colors)\n",
    "ax.legend(bbox_to_anchor=(0.5, 1.1), loc=\"upper center\", ncol=5, title=\"\", fontsize=6)\n",
    "\n",
    "ax.set_xlabel(\"Row index (each row is a peer connection)\")\n",
    "ax.set_ylabel(\"Share\")\n",
    "ax.set_title(\"Stacked Area Chart of Message Type Shares (Filtered)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verack count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final2[\"getdata_count\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Discard useless data\n",
    "\n",
    "Strip unnecessary data:\n",
    "\n",
    "- Tracepoint data includes `peer_id`, `peer_conn_type`, `peer_addr`, `flow`\n",
    "(traffic directory, i.e. in- or outbound), `msg_type` and `size`. Of these, only\n",
    "`flow`, `msg_type` and `size` are retained in their original form. A new `ipv6`\n",
    "column is introduced to indicate whether a message was sent via IPv4 or IPv6,\n",
    "since the version affects IP header sizes used for the traffic estimate.\n",
    "\n",
    "- Systemd IP accounting data includes rows for `IPIngressPackets`,\n",
    "`IPEgressPackets`, `IPIngressBytes`, and `IPEgressBytes`. Packet data is\n",
    "discarded and byte data is converted from absolute to relative (i.e. from bytes\n",
    "since measurement was started to bytes since previous row/sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "import numpy as np\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "\n",
    "df_tp = df_tp.dropna()\n",
    "df_tp[\"ipv6\"] = df_tp[\"peer_addr\"].parallel_apply(lambda x: True if \"[\" in x else False)\n",
    "df_tp = df_tp[[\"ipv6\", \"flow\", \"msg_type\", \"size\"]]\n",
    "\n",
    "df_emp = df_emp.dropna()\n",
    "df_emp = df_emp[[\"IPIngressBytes\", \"IPEgressBytes\"]]\n",
    "df_emp = df_emp.diff()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Estimate TCP/IP traffic from message sizes\n",
    "\n",
    "TCP/IP traffic is estimated using the following assumptions:\n",
    "- MTU size is 1500 bytes (common default)\n",
    "- Bitcoin protocol overhead is 24 bytes (4-byte magic, 12-byte command, 4-byte\n",
    "  each for payload length and checksum)\n",
    "- TCP header size of 32 bytes, comprising 20-byte minimum TCP header size plus 10-byte timestamps option (used by default by the Linux kernel to make real-time round-trip measurements) and two padding bytes to align options to 32-bit boundaries\n",
    "- IPv4 and v6 header sizes of 20 and 40 bytes (default)\n",
    "\n",
    "The estimate uses the following approach. First, the application-level message\n",
    "size is computed by adding the Bitcoin P2P message overhead to the message size.\n",
    "Next, the number of TCP segments is computed by dividing the application-level\n",
    "size obtained during the previous step by the maximum segment size (which\n",
    "corresponds to the MTU minus TCP and IP headers) to compute the number of TCP\n",
    "segments. Then, the total TCP/IP overhead is computed (number of segments times\n",
    "TCP and IP header overhead). Moreover, the overhead of ACKs is estimated to be\n",
    "half of the number of segments times the sum of IP and TCP header sizes, since\n",
    "generally ACKs are sent for every two packets.  Finally, TCP/IP traffic is\n",
    "estimated by combining the application-level message size with the total TCP/IP\n",
    "and ACK overhead.\n",
    "\n",
    "Next, empirical TCP/IP measurements obtained via systemd accounting are combined\n",
    "with the estimate so the latter can be validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "\n",
    "def estimate_network_traffic(row):\n",
    "    MAX_MTU_SIZE = 1500\n",
    "    BITCOIN_PROTOCOL_OVERHEAD = 24\n",
    "    TCP_HEADER_SIZE = 32\n",
    "    IP_HEADER_SIZE = 40 if row[\"ipv6\"] else 20\n",
    "    ACK_RATIO = 2\n",
    "    MSS = MAX_MTU_SIZE - IP_HEADER_SIZE - TCP_HEADER_SIZE\n",
    "    bitcoin_message_size = row[\"size\"] + BITCOIN_PROTOCOL_OVERHEAD\n",
    "    num_segments = math.ceil(bitcoin_message_size / MSS)\n",
    "    tcpip_overhead = num_segments * (IP_HEADER_SIZE + TCP_HEADER_SIZE)\n",
    "    ack_overhead = (num_segments / ACK_RATIO) * (IP_HEADER_SIZE + TCP_HEADER_SIZE)\n",
    "    return bitcoin_message_size + tcpip_overhead + ack_overhead\n",
    "\n",
    "\n",
    "df_tp[\"net_size\"] = df_tp.parallel_apply(estimate_network_traffic, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Aggregate data (to hourly and daily granularity)\n",
    "\n",
    "First, the dataframe contaiing empirical data from systemd's IP accounting is\n",
    "pivoted so it can be aggregated.\n",
    "\n",
    "Next, the pivoted df and the tracepoint df are aggregated to produce hourly and\n",
    "daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp = (\n",
    "    df_emp.rename(columns={\"IPIngressBytes\": \"in\", \"IPEgressBytes\": \"out\"})[\n",
    "        [\"in\", \"out\"]\n",
    "    ]\n",
    "    .stack()\n",
    "    .rename(\"net_size\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_1\": \"flow\"})\n",
    "    .set_index(\"timestamp\")\n",
    ")\n",
    "\n",
    "\n",
    "def agg_sum(df, cols, freq, data=\"net_size\"):\n",
    "    \"\"\"Aggregate 'data' col based on datetime index with frequency 'freq', using\n",
    "    summation using 'cols' as differentiator.\"\"\"\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp.index = df_tmp.index.floor(freq)\n",
    "    df_result = (\n",
    "        df_tmp.groupby([\"timestamp\"] + cols)[data]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .set_index(\"timestamp\")\n",
    "    )\n",
    "    return df_result\n",
    "\n",
    "\n",
    "dfs = {\n",
    "    \"est_hourly\": agg_sum(df_tp, [\"flow\", \"msg_type\"], freq=\"1h\"),\n",
    "    \"est_daily\": agg_sum(df_tp, [\"flow\", \"msg_type\"], freq=\"1d\"),\n",
    "    \"emp_hourly\": agg_sum(df_emp, [\"flow\"], freq=\"1h\"),\n",
    "    \"emp_daily\": agg_sum(df_emp, [\"flow\"], freq=\"1d\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Format aggregated data\n",
    "\n",
    "Pivot `flow` column of dataframes to get `in` and `out` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot(df, index, columns=\"flow\", values=\"net_size\"):\n",
    "    \"\"\"Pivot dataframe: keep 'index' as rows, 'columns' as columns and 'values'\n",
    "    as values.  Set 'timestamp' as new index, fill missing values with zero and\n",
    "    convert new cols to int.\"\"\"\n",
    "\n",
    "    return (\n",
    "        df.reset_index()\n",
    "        .pivot(\n",
    "            index=index,\n",
    "            columns=columns,\n",
    "            values=values,\n",
    "        )\n",
    "        .rename_axis(None, axis=1)\n",
    "        .reset_index()\n",
    "        .set_index(\"timestamp\")\n",
    "        .fillna(0)\n",
    "        .astype({\"in\": \"int\", \"out\": \"int\"})\n",
    "    )\n",
    "\n",
    "\n",
    "dfs = {\n",
    "    \"est_hourly\": pivot(dfs[\"est_hourly\"], [\"timestamp\", \"msg_type\"]),\n",
    "    \"est_daily\": pivot(dfs[\"est_daily\"], [\"timestamp\", \"msg_type\"]),\n",
    "    \"emp_hourly\": pivot(dfs[\"emp_hourly\"], [\"timestamp\"]),\n",
    "    \"emp_daily\": pivot(dfs[\"emp_daily\"], [\"timestamp\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Sanitize data\n",
    "\n",
    "Whenever the `nix-bitcoin-monitor` systemd service (which performs the data\n",
    "collection) is restarted, the IP accounting counters are reset to zero. As a\n",
    "result, `diff()`ing consecutive readings is going to break (think large valule\n",
    "in previous row followed by small value in next row, leading to negative\n",
    "values). This is addressed by setting values smaller than zero to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dfs.items():\n",
    "    if not name.startswith(\"emp_\"):\n",
    "        continue\n",
    "    for row in [\"in\", \"out\"]:\n",
    "        df.loc[df[row] < 0, row] = 0\n",
    "    dfs[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "Store transformation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "if not data_dir.exists():\n",
    "    data_dir.mkdir()\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    df.to_csv(f\"data/data_{name}.csv.bz2\", compression=\"bz2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
