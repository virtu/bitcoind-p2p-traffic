{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract\n",
    "\n",
    "Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "SOURCE_HOST = \"tau\"\n",
    "\n",
    "\n",
    "def read_csvs(directory: str) -> pd.DataFrame:\n",
    "    files = sorted(glob.glob(os.path.join(directory, \"*.csv\")))\n",
    "    df_list = [pd.read_csv(file, index_col=0, parse_dates=True) for file in files]\n",
    "    return pd.concat(df_list).sort_index()\n",
    "\n",
    "\n",
    "df_emp = read_csvs(f\"../../input_data/{SOURCE_HOST}/systemd/ip_accounting/\")\n",
    "df_tp = read_csvs(f\"../../input_data/{SOURCE_HOST}/tracepoints/net/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Discard useless data\n",
    "\n",
    "Strip unnecessary data:\n",
    "\n",
    "- Tracepoint data includes `peer_id`, `peer_conn_type`, `peer_addr`, `flow`\n",
    "(traffic directory, i.e. in- or outbound), `msg_type` and `size`. Of these, only\n",
    "`flow`, `msg_type` and `size` are retained in their original form. A new `ipv6`\n",
    "column is introduced to indicate whether a message was sent via IPv4 or IPv6,\n",
    "since the version affects IP header sizes used for the traffic estimate.\n",
    "\n",
    "- Systemd IP accounting data includes rows for `IPIngressPackets`,\n",
    "`IPEgressPackets`, `IPIngressBytes`, and `IPEgressBytes`. Packet data is\n",
    "discarded and byte data is converted from absolute to relative (i.e. from bytes\n",
    "since measurement was started to bytes since previous row/sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "import numpy as np\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "\n",
    "df_tp = df_tp.dropna()\n",
    "df_tp[\"ipv6\"] = df_tp[\"peer_addr\"].parallel_apply(lambda x: True if \"[\" in x else False)\n",
    "df_tp = df_tp[[\"ipv6\", \"flow\", \"msg_type\", \"size\"]]\n",
    "\n",
    "df_emp = df_emp.dropna()\n",
    "df_emp = df_emp[[\"IPIngressBytes\", \"IPEgressBytes\"]]\n",
    "df_emp = df_emp.diff()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Estimate TCP/IP traffic from message sizes\n",
    "\n",
    "TCP/IP traffic is estimated using the following assumptions:\n",
    "- MTU size is 1500 bytes (common default)\n",
    "- Bitcoin protocol overhead is 24 bytes (4-byte magic, 12-byte command, 4-byte\n",
    "  each for payload length and checksum)\n",
    "- TCP header size of 32 bytes, comprising 20-byte minimum TCP header size plus 10-byte timestamps option (used by default by the Linux kernel to make real-time round-trip measurements) and two padding bytes to align options to 32-bit boundaries\n",
    "- IPv4 and v6 header sizes of 20 and 40 bytes (default)\n",
    "\n",
    "The estimate uses the following approach. First, the application-level message\n",
    "size is computed by adding the Bitcoin P2P message overhead to the message size.\n",
    "Next, the number of TCP segments is computed by dividing the application-level\n",
    "size obtained during the previous step by the maximum segment size (which\n",
    "corresponds to the MTU minus TCP and IP headers) to compute the number of TCP\n",
    "segments. Then, the total TCP/IP overhead is computed (number of segments times\n",
    "TCP and IP header overhead). Moreover, the overhead of ACKs is estimated to be\n",
    "half of the number of segments times the sum of IP and TCP header sizes, since\n",
    "generally ACKs are sent for every two packets.  Finally, TCP/IP traffic is\n",
    "estimated by combining the application-level message size with the total TCP/IP\n",
    "and ACK overhead.\n",
    "\n",
    "Next, empirical TCP/IP measurements obtained via systemd accounting are combined\n",
    "with the estimate so the latter can be validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "\n",
    "def estimate_network_traffic(row):\n",
    "    MAX_MTU_SIZE = 1500\n",
    "    BITCOIN_PROTOCOL_OVERHEAD = 24\n",
    "    TCP_HEADER_SIZE = 32\n",
    "    IP_HEADER_SIZE = 40 if row[\"ipv6\"] else 20\n",
    "    ACK_RATIO = 2\n",
    "    MSS = MAX_MTU_SIZE - IP_HEADER_SIZE - TCP_HEADER_SIZE\n",
    "    bitcoin_message_size = row[\"size\"] + BITCOIN_PROTOCOL_OVERHEAD\n",
    "    num_segments = math.ceil(bitcoin_message_size / MSS)\n",
    "    tcpip_overhead = num_segments * (IP_HEADER_SIZE + TCP_HEADER_SIZE)\n",
    "    ack_overhead = (num_segments / ACK_RATIO) * (IP_HEADER_SIZE + TCP_HEADER_SIZE)\n",
    "    return bitcoin_message_size + tcpip_overhead + ack_overhead\n",
    "\n",
    "\n",
    "df_tp[\"net_size\"] = df_tp.parallel_apply(estimate_network_traffic, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Aggregate data (to hourly and daily granularity)\n",
    "\n",
    "First, the dataframe contaiing empirical data from systemd's IP accounting is\n",
    "pivoted so it can be aggregated.\n",
    "\n",
    "Next, the pivoted df and the tracepoint df are aggregated to produce hourly and\n",
    "daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp = (\n",
    "    df_emp.rename(columns={\"IPIngressBytes\": \"in\", \"IPEgressBytes\": \"out\"})[\n",
    "        [\"in\", \"out\"]\n",
    "    ]\n",
    "    .stack()\n",
    "    .rename(\"net_size\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_1\": \"flow\"})\n",
    "    .set_index(\"timestamp\")\n",
    ")\n",
    "\n",
    "\n",
    "def agg_sum(df, cols, freq, data=\"net_size\"):\n",
    "    \"\"\"Aggregate 'data' col based on datetime index with frequency 'freq', using\n",
    "    summation using 'cols' as differentiator.\"\"\"\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp.index = df_tmp.index.floor(freq)\n",
    "    df_result = (\n",
    "        df_tmp.groupby([\"timestamp\"] + cols)[data]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .set_index(\"timestamp\")\n",
    "    )\n",
    "    return df_result\n",
    "\n",
    "\n",
    "dfs = {\n",
    "    \"est_hourly\": agg_sum(df_tp, [\"flow\", \"msg_type\"], freq=\"1h\"),\n",
    "    \"est_daily\": agg_sum(df_tp, [\"flow\", \"msg_type\"], freq=\"1d\"),\n",
    "    \"emp_hourly\": agg_sum(df_emp, [\"flow\"], freq=\"1h\"),\n",
    "    \"emp_daily\": agg_sum(df_emp, [\"flow\"], freq=\"1d\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Format aggregated data\n",
    "\n",
    "Pivot `flow` column of dataframes to get `in` and `out` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot(df, index, columns=\"flow\", values=\"net_size\"):\n",
    "    \"\"\"Pivot dataframe: keep 'index' as rows, 'columns' as columns and 'values'\n",
    "    as values.  Set 'timestamp' as new index, fill missing values with zero and\n",
    "    convert new cols to int.\"\"\"\n",
    "\n",
    "    return (\n",
    "        df.reset_index()\n",
    "        .pivot(\n",
    "            index=index,\n",
    "            columns=columns,\n",
    "            values=values,\n",
    "        )\n",
    "        .rename_axis(None, axis=1)\n",
    "        .reset_index()\n",
    "        .set_index(\"timestamp\")\n",
    "        .fillna(0)\n",
    "        .astype({\"in\": \"int\", \"out\": \"int\"})\n",
    "    )\n",
    "\n",
    "\n",
    "dfs = {\n",
    "    \"est_hourly\": pivot(dfs[\"est_hourly\"], [\"timestamp\", \"msg_type\"]),\n",
    "    \"est_daily\": pivot(dfs[\"est_daily\"], [\"timestamp\", \"msg_type\"]),\n",
    "    \"emp_hourly\": pivot(dfs[\"emp_hourly\"], [\"timestamp\"]),\n",
    "    \"emp_daily\": pivot(dfs[\"emp_daily\"], [\"timestamp\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Sanitize data\n",
    "\n",
    "Whenever the `nix-bitcoin-monitor` systemd service (which performs the data\n",
    "collection) is restarted, the IP accounting counters are reset to zero. As a\n",
    "result, `diff()`ing consecutive readings is going to break (think large valule\n",
    "in previous row followed by small value in next row, leading to negative\n",
    "values). This is addressed by setting values smaller than zero to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dfs.items():\n",
    "    if not name.startswith(\"emp_\"):\n",
    "        continue\n",
    "    for row in [\"in\", \"out\"]:\n",
    "        df.loc[df[row] < 0, row] = 0\n",
    "    dfs[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "Store transformation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "if not data_dir.exists():\n",
    "    data_dir.mkdir()\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    df.to_csv(f\"data/data_{name}.csv.bz2\", compression=\"bz2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
